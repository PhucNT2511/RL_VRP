{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8977435,"sourceType":"datasetVersion","datasetId":5405567},{"sourceId":8978989,"sourceType":"datasetVersion","datasetId":5406696}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!python3 '/kaggle/input/vrp-rl5/project2_2/main.py'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-17T20:38:53.673004Z","iopub.execute_input":"2024-07-17T20:38:53.674085Z","iopub.status.idle":"2024-07-17T22:53:01.034293Z","shell.execute_reply.started":"2024-07-17T20:38:53.674046Z","shell.execute_reply":"2024-07-17T22:53:01.033055Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Detected device cuda\nStarting VRP training\nTrain data: <Tasks.vrp.VehicleRoutingDataset object at 0x7d15d47c3850>\nActor: DRL4VRP(\n  (static_embedding): Embedding(\n    (conv): Conv1d(2, 128, kernel_size=(1,), stride=(1,))\n  )\n  (dynamic_embedding): Embedding(\n    (conv): Conv1d(2, 128, kernel_size=(1,), stride=(1,))\n  )\n  (decoder): Embedding(\n    (conv): Conv1d(2, 128, kernel_size=(1,), stride=(1,))\n  )\n  (pointer): Pointer(\n    (gru): GRU(128, 128, batch_first=True)\n    (embedding_attn): Attention()\n    (drop_rnn): Dropout(p=0.1, inplace=False)\n    (drop_hh): Dropout(p=0.1, inplace=False)\n  )\n) \nCritic: StateCritic(\n  (static_embedding): Embedding(\n    (conv): Conv1d(2, 128, kernel_size=(1,), stride=(1,))\n  )\n  (dynamic_embedding): Embedding(\n    (conv): Conv1d(2, 128, kernel_size=(1,), stride=(1,))\n  )\n  (fc1): Conv1d(256, 20, kernel_size=(1,), stride=(1,))\n  (fc2): Conv1d(20, 20, kernel_size=(1,), stride=(1,))\n  (fc3): Conv1d(20, 1, kernel_size=(1,), stride=(1,))\n)\nStarting training\n/kaggle/input/vrp-rl5/project2_2/Tasks/vrp.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return torch.tensor(tensor.data, device=dynamic.device)\n  Batch 99/3907, reward: 7.220, loss: -3.2843, took: 23.9082s\n  Batch 199/3907, reward: 7.200, loss: -0.1096, took: 22.9067s\n  Batch 299/3907, reward: 6.759, loss: -0.2728, took: 22.6439s\n  Batch 399/3907, reward: 5.937, loss: -0.2719, took: 21.7672s\n  Batch 499/3907, reward: 5.657, loss: -0.1141, took: 21.2195s\n  Batch 599/3907, reward: 5.572, loss: -0.0468, took: 21.2996s\n  Batch 699/3907, reward: 5.522, loss: -0.0737, took: 20.9628s\n  Batch 799/3907, reward: 5.479, loss: -0.0712, took: 20.7006s\n  Batch 899/3907, reward: 5.453, loss: -0.1031, took: 20.8138s\n  Batch 999/3907, reward: 5.435, loss: -0.0697, took: 20.6705s\n  Batch 1099/3907, reward: 5.434, loss: 0.0104, took: 20.5868s\n  Batch 1199/3907, reward: 5.406, loss: -0.0800, took: 20.5552s\n  Batch 1299/3907, reward: 5.403, loss: -0.1388, took: 20.5517s\n  Batch 1399/3907, reward: 5.400, loss: -0.1052, took: 20.4878s\n  Batch 1499/3907, reward: 5.382, loss: -0.1415, took: 20.6462s\n  Batch 1599/3907, reward: 5.398, loss: -0.1054, took: 20.2032s\n  Batch 1699/3907, reward: 5.371, loss: -0.1136, took: 20.2958s\n  Batch 1799/3907, reward: 5.358, loss: -0.2077, took: 20.4554s\n  Batch 1899/3907, reward: 5.357, loss: -0.1724, took: 20.4101s\n  Batch 1999/3907, reward: 5.359, loss: -0.1054, took: 20.3287s\n  Batch 2099/3907, reward: 5.346, loss: -0.1680, took: 20.4668s\n  Batch 2199/3907, reward: 5.353, loss: -0.1732, took: 20.2440s\n  Batch 2299/3907, reward: 5.343, loss: -0.1466, took: 20.4896s\n  Batch 2399/3907, reward: 5.348, loss: -0.2992, took: 20.4787s\n  Batch 2499/3907, reward: 5.340, loss: -0.2096, took: 20.3966s\n  Batch 2599/3907, reward: 5.345, loss: -0.1573, took: 20.4547s\n  Batch 2699/3907, reward: 5.338, loss: -0.1553, took: 20.3200s\n  Batch 2799/3907, reward: 5.332, loss: -0.1521, took: 20.3507s\n  Batch 2899/3907, reward: 5.328, loss: -0.1751, took: 20.4304s\n  Batch 2999/3907, reward: 5.336, loss: -0.1582, took: 20.3425s\n  Batch 3099/3907, reward: 5.336, loss: -0.1481, took: 20.3402s\n  Batch 3199/3907, reward: 5.304, loss: -0.1816, took: 20.3601s\n  Batch 3299/3907, reward: 5.323, loss: -0.1794, took: 20.4365s\n  Batch 3399/3907, reward: 5.312, loss: -0.2276, took: 20.7197s\n  Batch 3499/3907, reward: 5.306, loss: -0.1953, took: 20.5253s\n  Batch 3599/3907, reward: 5.297, loss: -0.1334, took: 20.2368s\n  Batch 3699/3907, reward: 5.304, loss: -0.2091, took: 20.1860s\n  Batch 3799/3907, reward: 5.304, loss: -0.1493, took: 20.5229s\n  Batch 3899/3907, reward: 5.300, loss: -0.1334, took: 20.4057s\ntensor([ 6,  5,  9,  3,  1,  4,  0, 10,  8,  7,  0,  2,  0,  0],\n       device='cuda:0')\ntensor([ 2,  4,  6,  3,  0, 10,  1,  8,  5,  9,  0,  7,  0,  0,  0,  0],\n       device='cuda:0')\ntensor([ 7,  6,  9,  2, 10,  0,  3,  4,  5,  0,  8,  1,  0,  0,  0],\n       device='cuda:0')\ntensor([ 9,  5,  7,  4,  2, 10,  0,  1,  6,  3,  0,  8,  0,  0,  0],\n       device='cuda:0')\nMean epoch loss/reward: -0.2296, 5.5201, 5.2099, took: 817.7189s (20.7467s / 100 batches)\n\n  Batch 99/3907, reward: 5.294, loss: -0.1778, took: 20.4174s\n  Batch 199/3907, reward: 5.303, loss: -0.2306, took: 20.3966s\n  Batch 299/3907, reward: 5.299, loss: -0.1501, took: 20.5095s\n  Batch 399/3907, reward: 5.305, loss: -0.1855, took: 20.6005s\n  Batch 499/3907, reward: 5.292, loss: -0.2133, took: 20.4855s\n  Batch 599/3907, reward: 5.299, loss: -0.2507, took: 20.6257s\n  Batch 699/3907, reward: 5.281, loss: -0.1769, took: 20.7099s\n  Batch 799/3907, reward: 5.283, loss: -0.1516, took: 20.5409s\n  Batch 899/3907, reward: 5.289, loss: -0.1799, took: 20.5969s\n  Batch 999/3907, reward: 5.268, loss: -0.1552, took: 20.5230s\n  Batch 1099/3907, reward: 5.284, loss: -0.1617, took: 20.6581s\n  Batch 1199/3907, reward: 5.278, loss: -0.1873, took: 20.7657s\n  Batch 1299/3907, reward: 5.279, loss: -0.1985, took: 20.4428s\n  Batch 1399/3907, reward: 5.282, loss: -0.1254, took: 20.4350s\n  Batch 1499/3907, reward: 5.265, loss: -0.0861, took: 20.6914s\n  Batch 1599/3907, reward: 5.276, loss: -0.1333, took: 20.6710s\n  Batch 1699/3907, reward: 5.274, loss: -0.1459, took: 20.6456s\n  Batch 1799/3907, reward: 5.274, loss: -0.1458, took: 20.5981s\n  Batch 1899/3907, reward: 5.263, loss: -0.1271, took: 20.5635s\n  Batch 1999/3907, reward: 5.263, loss: -0.1305, took: 20.5364s\n  Batch 2099/3907, reward: 5.269, loss: -0.2078, took: 20.4979s\n  Batch 2199/3907, reward: 5.274, loss: -0.1733, took: 20.4537s\n  Batch 2299/3907, reward: 5.261, loss: -0.2182, took: 20.6120s\n  Batch 2399/3907, reward: 5.270, loss: -0.1749, took: 20.6547s\n  Batch 2499/3907, reward: 5.249, loss: -0.1627, took: 20.7447s\n  Batch 2599/3907, reward: 5.259, loss: -0.2279, took: 20.6360s\n  Batch 2699/3907, reward: 5.263, loss: -0.2412, took: 20.5531s\n  Batch 2799/3907, reward: 5.260, loss: -0.1409, took: 20.7872s\n  Batch 2899/3907, reward: 5.251, loss: -0.1470, took: 20.6261s\n  Batch 2999/3907, reward: 5.264, loss: -0.1090, took: 20.6411s\n  Batch 3099/3907, reward: 5.266, loss: -0.1660, took: 20.5821s\n  Batch 3199/3907, reward: 5.242, loss: -0.2238, took: 20.6290s\n  Batch 3299/3907, reward: 5.244, loss: -0.1643, took: 20.5266s\n  Batch 3399/3907, reward: 5.247, loss: -0.1778, took: 20.7685s\n  Batch 3499/3907, reward: 5.254, loss: -0.2011, took: 20.6572s\n  Batch 3599/3907, reward: 5.244, loss: -0.1757, took: 20.5407s\n  Batch 3699/3907, reward: 5.240, loss: -0.1719, took: 20.6768s\n  Batch 3799/3907, reward: 5.277, loss: -0.2649, took: 20.5869s\n  Batch 3899/3907, reward: 5.282, loss: -0.2260, took: 20.9191s\ntensor([ 6,  5,  9,  3,  1,  4,  0, 10,  8,  7,  0,  2,  0,  0,  0],\n       device='cuda:0')\ntensor([ 2,  4,  6,  3,  0, 10,  1,  8,  5,  9,  0,  7,  0,  0,  0,  0,  0],\n       device='cuda:0')\ntensor([ 7,  6,  9,  2, 10,  0,  3,  4,  5,  0,  8,  1,  0,  0,  0],\n       device='cuda:0')\ntensor([ 9,  7,  5,  2,  4, 10,  0,  1,  6,  8,  0,  3,  0,  0,  0],\n       device='cuda:0')\nMean epoch loss/reward: -0.1765, 5.2710, 5.1989, took: 812.4762s (20.6027s / 100 batches)\n\n  Batch 99/3907, reward: 5.253, loss: -0.1660, took: 20.7644s\n  Batch 199/3907, reward: 5.244, loss: -0.1187, took: 20.6550s\n  Batch 299/3907, reward: 5.261, loss: -0.1831, took: 20.6149s\n  Batch 399/3907, reward: 5.258, loss: -0.1636, took: 20.5660s\n  Batch 499/3907, reward: 5.250, loss: -0.1592, took: 20.6175s\n  Batch 599/3907, reward: 5.259, loss: -0.1743, took: 20.5684s\n  Batch 699/3907, reward: 5.254, loss: -0.1638, took: 20.5481s\n  Batch 799/3907, reward: 5.246, loss: -0.1629, took: 20.5741s\n  Batch 899/3907, reward: 5.247, loss: -0.2229, took: 20.5818s\n  Batch 999/3907, reward: 5.254, loss: -0.1909, took: 20.7673s\n  Batch 1099/3907, reward: 5.241, loss: -0.2194, took: 20.6162s\n  Batch 1199/3907, reward: 5.235, loss: -0.1882, took: 20.4730s\n  Batch 1299/3907, reward: 5.229, loss: -0.1954, took: 20.5023s\n  Batch 1399/3907, reward: 5.239, loss: -0.2034, took: 20.5573s\n  Batch 1499/3907, reward: 5.228, loss: -0.1349, took: 20.6198s\n  Batch 1599/3907, reward: 5.235, loss: -0.2051, took: 20.5413s\n  Batch 1699/3907, reward: 5.235, loss: -0.1728, took: 20.6237s\n  Batch 1799/3907, reward: 5.238, loss: -0.1850, took: 20.6602s\n  Batch 1899/3907, reward: 5.228, loss: -0.1761, took: 20.4549s\n  Batch 1999/3907, reward: 5.233, loss: -0.1766, took: 20.6870s\n  Batch 2099/3907, reward: 5.229, loss: -0.1678, took: 20.6034s\n  Batch 2199/3907, reward: 5.237, loss: -0.1998, took: 20.5332s\n  Batch 2299/3907, reward: 5.235, loss: -0.1186, took: 20.5664s\n  Batch 2399/3907, reward: 5.237, loss: -0.1668, took: 20.6617s\n  Batch 2499/3907, reward: 5.235, loss: -0.2041, took: 20.5354s\n  Batch 2599/3907, reward: 5.237, loss: -0.1670, took: 20.5294s\n  Batch 2699/3907, reward: 5.224, loss: -0.1761, took: 20.5043s\n  Batch 2799/3907, reward: 5.213, loss: -0.1753, took: 20.4491s\n  Batch 2899/3907, reward: 5.217, loss: -0.1999, took: 20.5515s\n  Batch 2999/3907, reward: 5.228, loss: -0.1077, took: 20.4860s\n  Batch 3099/3907, reward: 5.220, loss: -0.1685, took: 20.4560s\n  Batch 3199/3907, reward: 5.204, loss: -0.1693, took: 20.4392s\n  Batch 3299/3907, reward: 5.222, loss: -0.1370, took: 20.4463s\n  Batch 3399/3907, reward: 5.222, loss: -0.1538, took: 20.6006s\n  Batch 3499/3907, reward: 5.229, loss: -0.1952, took: 20.5098s\n  Batch 3599/3907, reward: 5.215, loss: -0.2009, took: 20.4319s\n  Batch 3699/3907, reward: 5.219, loss: -0.1493, took: 20.5596s\n  Batch 3799/3907, reward: 5.225, loss: -0.1791, took: 20.6905s\n  Batch 3899/3907, reward: 5.210, loss: -0.1947, took: 20.6793s\ntensor([ 6,  9,  5,  3,  1,  4,  0, 10,  8,  7,  0,  2,  0,  0],\n       device='cuda:0')\ntensor([ 2,  4,  6,  3,  0, 10,  1,  8,  5,  9,  0,  7,  0,  0,  0],\n       device='cuda:0')\ntensor([ 7,  6,  9,  2, 10,  0,  3,  4,  5,  0,  8,  1,  0,  0,  0],\n       device='cuda:0')\ntensor([ 9,  7,  5,  4,  2, 10,  0,  1,  6,  8,  0,  3,  0,  0,  0],\n       device='cuda:0')\nMean epoch loss/reward: -0.1743, 5.2340, 5.1346, took: 811.6741s (20.5699s / 100 batches)\n\n  Batch 99/3907, reward: 5.221, loss: -0.1766, took: 20.6198s\n  Batch 199/3907, reward: 5.218, loss: -0.1360, took: 20.5012s\n  Batch 299/3907, reward: 5.216, loss: -0.1538, took: 20.6038s\n  Batch 399/3907, reward: 5.216, loss: -0.1598, took: 20.5340s\n  Batch 499/3907, reward: 5.212, loss: -0.1676, took: 20.6122s\n  Batch 599/3907, reward: 5.206, loss: -0.1105, took: 20.4770s\n  Batch 699/3907, reward: 5.212, loss: -0.1394, took: 20.4183s\n  Batch 799/3907, reward: 5.198, loss: -0.1783, took: 20.5004s\n  Batch 899/3907, reward: 5.209, loss: -0.1633, took: 20.5211s\n  Batch 999/3907, reward: 5.201, loss: -0.1568, took: 20.4173s\n  Batch 1099/3907, reward: 5.206, loss: -0.1880, took: 20.6534s\n  Batch 1199/3907, reward: 5.209, loss: -0.1096, took: 20.4832s\n  Batch 1299/3907, reward: 5.205, loss: -0.1491, took: 20.3504s\n  Batch 1399/3907, reward: 5.219, loss: -0.1733, took: 20.6396s\n  Batch 1499/3907, reward: 5.196, loss: -0.1971, took: 20.5735s\n  Batch 1599/3907, reward: 5.202, loss: -0.1839, took: 20.6420s\n  Batch 1699/3907, reward: 5.209, loss: -0.1484, took: 20.5649s\n  Batch 1799/3907, reward: 5.203, loss: -0.1403, took: 20.5308s\n  Batch 1899/3907, reward: 5.203, loss: -0.2290, took: 20.5662s\n  Batch 1999/3907, reward: 5.198, loss: -0.1553, took: 20.4061s\n  Batch 2099/3907, reward: 5.205, loss: -0.1491, took: 20.3970s\n  Batch 2199/3907, reward: 5.211, loss: -0.1507, took: 20.2594s\n  Batch 2299/3907, reward: 5.223, loss: -0.1940, took: 20.6287s\n  Batch 2399/3907, reward: 5.203, loss: -0.1391, took: 20.3813s\n  Batch 2499/3907, reward: 5.203, loss: -0.1511, took: 20.6856s\n  Batch 2599/3907, reward: 5.201, loss: -0.1482, took: 20.4884s\n  Batch 2699/3907, reward: 5.204, loss: -0.1584, took: 20.4442s\n  Batch 2799/3907, reward: 5.197, loss: -0.1387, took: 20.5468s\n  Batch 2899/3907, reward: 5.195, loss: -0.1211, took: 20.5247s\n  Batch 2999/3907, reward: 5.205, loss: -0.1586, took: 20.3912s\n  Batch 3099/3907, reward: 5.182, loss: -0.1630, took: 20.6109s\n  Batch 3199/3907, reward: 5.180, loss: -0.1720, took: 20.3944s\n  Batch 3299/3907, reward: 5.197, loss: -0.1349, took: 20.4146s\n  Batch 3399/3907, reward: 5.193, loss: -0.1317, took: 20.3629s\n  Batch 3499/3907, reward: 5.194, loss: -0.1339, took: 20.6359s\n  Batch 3599/3907, reward: 5.186, loss: -0.1900, took: 20.3510s\n  Batch 3699/3907, reward: 5.209, loss: -0.1484, took: 20.4193s\n  Batch 3799/3907, reward: 5.192, loss: -0.1218, took: 20.5569s\n  Batch 3899/3907, reward: 5.212, loss: -0.1436, took: 20.3644s\ntensor([ 9,  6,  5,  3,  1,  4,  0, 10,  8,  7,  0,  2,  0,  0],\n       device='cuda:0')\ntensor([ 2,  4,  6,  3,  0, 10,  1,  8,  5,  9,  0,  7,  0,  0,  0],\n       device='cuda:0')\ntensor([ 7,  6,  9,  2, 10,  0,  4,  5,  3,  0,  8,  1,  0,  0,  0],\n       device='cuda:0')\ntensor([ 9,  7,  5,  2,  4, 10,  0,  1,  6,  3,  0,  8,  0,  0,  0],\n       device='cuda:0')\nMean epoch loss/reward: -0.1556, 5.2039, 5.1155, took: 808.9391s (20.4993s / 100 batches)\n\n  Batch 99/3907, reward: 5.198, loss: -0.1505, took: 20.6370s\n  Batch 199/3907, reward: 5.202, loss: -0.1109, took: 20.4836s\n  Batch 299/3907, reward: 5.189, loss: -0.1183, took: 20.3910s\n  Batch 399/3907, reward: 5.209, loss: -0.1747, took: 20.5831s\n  Batch 499/3907, reward: 5.197, loss: -0.1134, took: 20.5594s\n  Batch 599/3907, reward: 5.208, loss: -0.1885, took: 20.6255s\n  Batch 699/3907, reward: 5.191, loss: -0.1666, took: 20.6814s\n  Batch 799/3907, reward: 5.214, loss: -0.1825, took: 20.6114s\n  Batch 899/3907, reward: 5.201, loss: -0.1643, took: 20.5888s\n  Batch 999/3907, reward: 5.193, loss: -0.1453, took: 20.6862s\n  Batch 1099/3907, reward: 5.198, loss: -0.1223, took: 20.4644s\n  Batch 1199/3907, reward: 5.229, loss: -0.1607, took: 20.8822s\n  Batch 1299/3907, reward: 5.223, loss: -0.1372, took: 20.4862s\n  Batch 1399/3907, reward: 5.210, loss: -0.1370, took: 20.5203s\n  Batch 1499/3907, reward: 5.201, loss: -0.1203, took: 20.4484s\n  Batch 1599/3907, reward: 5.200, loss: -0.1407, took: 20.4899s\n  Batch 1699/3907, reward: 5.191, loss: -0.1543, took: 20.4388s\n  Batch 1799/3907, reward: 5.210, loss: -0.1753, took: 19.9789s\n  Batch 1899/3907, reward: 5.207, loss: -0.1624, took: 20.0474s\n  Batch 1999/3907, reward: 5.199, loss: -0.1811, took: 20.1901s\n  Batch 2099/3907, reward: 5.197, loss: -0.1618, took: 20.2468s\n  Batch 2199/3907, reward: 5.193, loss: -0.1762, took: 20.2951s\n  Batch 2299/3907, reward: 5.192, loss: -0.1810, took: 20.5772s\n  Batch 2399/3907, reward: 5.198, loss: -0.1697, took: 20.5288s\n  Batch 2499/3907, reward: 5.191, loss: -0.1936, took: 20.4862s\n  Batch 2599/3907, reward: 5.188, loss: -0.1274, took: 20.3941s\n  Batch 2699/3907, reward: 5.188, loss: -0.1622, took: 20.4483s\n  Batch 2799/3907, reward: 5.184, loss: -0.1970, took: 20.7223s\n  Batch 2899/3907, reward: 5.181, loss: -0.1683, took: 20.3635s\n  Batch 2999/3907, reward: 5.191, loss: -0.1491, took: 20.4680s\n  Batch 3099/3907, reward: 5.195, loss: -0.1661, took: 20.5047s\n  Batch 3199/3907, reward: 5.181, loss: -0.1822, took: 20.1138s\n  Batch 3299/3907, reward: 5.182, loss: -0.1747, took: 20.3277s\n  Batch 3399/3907, reward: 5.178, loss: -0.1730, took: 20.3047s\n  Batch 3499/3907, reward: 5.185, loss: -0.1670, took: 20.3268s\n  Batch 3599/3907, reward: 5.178, loss: -0.1614, took: 20.3859s\n  Batch 3699/3907, reward: 5.206, loss: -0.1622, took: 20.1437s\n  Batch 3799/3907, reward: 5.170, loss: -0.1387, took: 20.1010s\n  Batch 3899/3907, reward: 5.182, loss: -0.1348, took: 20.5269s\ntensor([ 6,  9,  5,  3,  1,  4,  0, 10,  8,  7,  0,  2,  0,  0],\n       device='cuda:0')\ntensor([ 2,  4,  6,  3,  0, 10,  1,  8,  5,  9,  0,  7,  0,  0,  0,  0],\n       device='cuda:0')\ntensor([ 7,  6,  9,  2, 10,  0,  4,  5,  3,  0,  1,  8,  0,  0,  0],\n       device='cuda:0')\ntensor([ 9,  5,  7,  2,  4, 10,  0,  8,  1,  6,  0,  3,  0,  0,  0],\n       device='cuda:0')\nMean epoch loss/reward: -0.1578, 5.1956, 5.1196, took: 806.3483s (20.4374s / 100 batches)\n\n  Batch 99/3907, reward: 5.194, loss: -0.1647, took: 20.5920s\n  Batch 199/3907, reward: 5.176, loss: -0.1250, took: 20.3574s\n  Batch 299/3907, reward: 5.176, loss: -0.1003, took: 20.4517s\n  Batch 399/3907, reward: 5.170, loss: -0.1572, took: 20.5825s\n  Batch 499/3907, reward: 5.178, loss: -0.1520, took: 20.4917s\n  Batch 599/3907, reward: 5.168, loss: -0.1603, took: 20.4089s\n  Batch 699/3907, reward: 5.177, loss: -0.1246, took: 20.5662s\n  Batch 799/3907, reward: 5.181, loss: -0.1312, took: 19.9303s\n  Batch 899/3907, reward: 5.170, loss: -0.1617, took: 20.2383s\n  Batch 999/3907, reward: 5.162, loss: -0.1486, took: 20.1660s\n  Batch 1099/3907, reward: 5.167, loss: -0.1833, took: 20.3252s\n  Batch 1199/3907, reward: 5.174, loss: -0.1602, took: 20.5353s\n  Batch 1299/3907, reward: 5.163, loss: -0.1487, took: 20.3314s\n  Batch 1399/3907, reward: 5.164, loss: -0.1448, took: 20.3661s\n  Batch 1499/3907, reward: 5.152, loss: -0.0882, took: 20.4369s\n  Batch 1599/3907, reward: 5.166, loss: -0.1410, took: 20.2883s\n  Batch 1699/3907, reward: 5.168, loss: -0.1665, took: 20.2334s\n  Batch 1799/3907, reward: 5.153, loss: -0.1110, took: 20.3930s\n  Batch 1899/3907, reward: 5.166, loss: -0.1429, took: 20.4923s\n  Batch 1999/3907, reward: 5.166, loss: -0.1275, took: 20.3828s\n  Batch 2099/3907, reward: 5.174, loss: -0.1317, took: 20.5183s\n  Batch 2199/3907, reward: 5.163, loss: -0.1521, took: 20.5831s\n  Batch 2299/3907, reward: 5.159, loss: -0.1780, took: 20.5597s\n  Batch 2399/3907, reward: 5.157, loss: -0.1607, took: 20.3220s\n  Batch 2499/3907, reward: 5.168, loss: -0.1630, took: 20.3162s\n  Batch 2599/3907, reward: 5.171, loss: -0.1581, took: 20.2883s\n  Batch 2699/3907, reward: 5.164, loss: -0.1535, took: 20.0434s\n  Batch 2799/3907, reward: 5.166, loss: -0.1299, took: 20.3963s\n  Batch 2899/3907, reward: 5.165, loss: -0.1761, took: 20.1311s\n  Batch 2999/3907, reward: 5.158, loss: -0.1756, took: 20.2731s\n  Batch 3099/3907, reward: 5.155, loss: -0.1560, took: 20.4493s\n  Batch 3199/3907, reward: 5.171, loss: -0.1422, took: 20.6807s\n  Batch 3299/3907, reward: 5.174, loss: -0.1233, took: 20.5672s\n  Batch 3399/3907, reward: 5.186, loss: -0.1576, took: 20.4119s\n  Batch 3499/3907, reward: 5.210, loss: -0.1260, took: 20.4781s\n  Batch 3599/3907, reward: 5.199, loss: -0.1586, took: 20.4571s\n  Batch 3699/3907, reward: 5.203, loss: -0.1551, took: 20.4325s\n  Batch 3799/3907, reward: 5.201, loss: -0.1857, took: 20.5428s\n  Batch 3899/3907, reward: 5.196, loss: -0.1639, took: 20.4723s\ntensor([ 6,  9,  5,  3,  1,  4,  0,  8, 10,  7,  0,  2,  0,  0,  0],\n       device='cuda:0')\ntensor([ 2,  4,  6,  3,  0,  5, 10,  1,  8,  9,  0,  7,  0,  0,  0],\n       device='cuda:0')\ntensor([ 7,  6,  9,  2, 10,  0,  1,  4,  3,  0,  8,  5,  0,  0,  0],\n       device='cuda:0')\ntensor([ 9,  5,  4,  7,  2, 10,  0,  6,  1,  3,  0,  8,  0,  0,  0],\n       device='cuda:0')\nMean epoch loss/reward: -0.1485, 5.1726, 5.0987, took: 804.0936s (20.3973s / 100 batches)\n\n  Batch 99/3907, reward: 5.194, loss: -0.1506, took: 20.6087s\n  Batch 199/3907, reward: 5.179, loss: -0.1838, took: 20.2406s\n  Batch 299/3907, reward: 5.175, loss: -0.2014, took: 20.5808s\n  Batch 399/3907, reward: 5.181, loss: -0.2118, took: 20.3020s\n  Batch 499/3907, reward: 5.184, loss: -0.1915, took: 20.4027s\n  Batch 599/3907, reward: 5.185, loss: -0.1692, took: 20.5642s\n  Batch 699/3907, reward: 5.183, loss: -0.1638, took: 20.4084s\n  Batch 799/3907, reward: 5.184, loss: -0.1676, took: 20.6375s\n  Batch 899/3907, reward: 5.179, loss: -0.1851, took: 20.4835s\n  Batch 999/3907, reward: 5.189, loss: -0.1647, took: 20.5427s\n  Batch 1099/3907, reward: 5.174, loss: -0.1951, took: 20.5580s\n  Batch 1199/3907, reward: 5.183, loss: -0.1539, took: 20.0743s\n  Batch 1299/3907, reward: 5.170, loss: -0.1743, took: 20.5019s\n  Batch 1399/3907, reward: 5.179, loss: -0.1628, took: 20.4215s\n  Batch 1499/3907, reward: 5.186, loss: -0.1481, took: 20.4213s\n  Batch 1599/3907, reward: 5.178, loss: -0.1693, took: 20.3444s\n  Batch 1699/3907, reward: 5.177, loss: -0.1663, took: 20.6018s\n  Batch 1799/3907, reward: 5.186, loss: -0.1261, took: 20.4505s\n  Batch 1899/3907, reward: 5.178, loss: -0.1631, took: 20.2885s\n  Batch 1999/3907, reward: 5.181, loss: -0.1545, took: 20.2656s\n  Batch 2099/3907, reward: 5.168, loss: -0.1711, took: 20.2762s\n  Batch 2199/3907, reward: 5.165, loss: -0.1318, took: 20.2211s\n  Batch 2299/3907, reward: 5.163, loss: -0.1734, took: 20.2748s\n  Batch 2399/3907, reward: 5.159, loss: -0.1662, took: 20.3835s\n  Batch 2499/3907, reward: 5.159, loss: -0.1408, took: 19.8636s\n  Batch 2599/3907, reward: 5.177, loss: -0.1832, took: 20.2767s\n  Batch 2699/3907, reward: 5.160, loss: -0.0770, took: 20.7135s\n  Batch 2799/3907, reward: 5.162, loss: -0.1617, took: 20.2726s\n  Batch 2899/3907, reward: 5.157, loss: -0.1412, took: 20.2650s\n  Batch 2999/3907, reward: 5.160, loss: -0.1085, took: 20.2777s\n  Batch 3099/3907, reward: 5.165, loss: -0.1532, took: 20.3683s\n  Batch 3199/3907, reward: 5.170, loss: -0.1659, took: 20.3474s\n  Batch 3299/3907, reward: 5.165, loss: -0.1237, took: 20.3460s\n  Batch 3399/3907, reward: 5.169, loss: -0.1669, took: 20.3857s\n  Batch 3499/3907, reward: 5.162, loss: -0.1527, took: 20.0920s\n  Batch 3599/3907, reward: 5.146, loss: -0.1372, took: 19.8468s\n  Batch 3699/3907, reward: 5.150, loss: -0.2136, took: 20.1237s\n  Batch 3799/3907, reward: 5.153, loss: -0.1330, took: 20.2146s\n  Batch 3899/3907, reward: 5.148, loss: -0.1671, took: 19.9988s\ntensor([ 9,  5,  6,  1,  3,  4,  0, 10,  8,  7,  0,  2,  0,  0,  0],\n       device='cuda:0')\ntensor([ 2,  4,  6,  3,  0, 10,  1,  8,  5,  9,  0,  7,  0,  0,  0],\n       device='cuda:0')\ntensor([ 6,  9,  2,  4,  3,  0, 10,  5,  7,  0,  8,  1,  0,  0,  0],\n       device='cuda:0')\ntensor([ 9,  2,  5,  7,  1, 10,  0,  4,  6,  3,  0,  8,  0,  0,  0],\n       device='cuda:0')\nMean epoch loss/reward: -0.1605, 5.1713, 5.0814, took: 801.4319s (20.3397s / 100 batches)\n\n  Batch 99/3907, reward: 5.143, loss: -0.1512, took: 19.9387s\n  Batch 199/3907, reward: 5.147, loss: -0.1643, took: 20.2362s\n  Batch 299/3907, reward: 5.140, loss: -0.1662, took: 20.2943s\n  Batch 399/3907, reward: 5.142, loss: -0.1258, took: 20.3144s\n  Batch 499/3907, reward: 5.146, loss: -0.1707, took: 20.1915s\n  Batch 599/3907, reward: 5.143, loss: -0.1250, took: 20.2619s\n  Batch 699/3907, reward: 5.144, loss: -0.1453, took: 20.2307s\n  Batch 799/3907, reward: 5.141, loss: -0.1634, took: 20.3023s\n  Batch 899/3907, reward: 5.147, loss: -0.1560, took: 20.1825s\n  Batch 999/3907, reward: 5.145, loss: -0.1588, took: 20.3162s\n  Batch 1099/3907, reward: 5.141, loss: -0.1312, took: 20.2329s\n  Batch 1199/3907, reward: 5.131, loss: -0.1606, took: 20.2074s\n  Batch 1299/3907, reward: 5.146, loss: -0.1301, took: 20.3717s\n  Batch 1399/3907, reward: 5.147, loss: -0.1189, took: 20.2195s\n  Batch 1499/3907, reward: 5.158, loss: -0.1164, took: 20.2052s\n  Batch 1599/3907, reward: 5.134, loss: -0.1496, took: 20.2716s\n  Batch 1699/3907, reward: 5.145, loss: -0.1563, took: 20.1507s\n  Batch 1799/3907, reward: 5.146, loss: -0.2128, took: 19.8161s\n  Batch 1899/3907, reward: 5.138, loss: -0.1751, took: 19.7702s\n  Batch 1999/3907, reward: 5.130, loss: -0.1442, took: 19.9260s\n  Batch 2099/3907, reward: 5.127, loss: -0.1311, took: 20.2862s\n  Batch 2199/3907, reward: 5.132, loss: -0.1691, took: 20.1356s\n  Batch 2299/3907, reward: 5.137, loss: -0.1416, took: 20.1386s\n  Batch 2399/3907, reward: 5.138, loss: -0.1312, took: 20.1234s\n  Batch 2499/3907, reward: 5.143, loss: -0.1588, took: 19.8427s\n  Batch 2599/3907, reward: 5.126, loss: -0.1825, took: 19.7599s\n  Batch 2699/3907, reward: 5.130, loss: -0.1103, took: 20.3136s\n  Batch 2799/3907, reward: 5.127, loss: -0.1331, took: 20.0602s\n  Batch 2899/3907, reward: 5.127, loss: -0.1632, took: 20.2303s\n  Batch 2999/3907, reward: 5.133, loss: -0.1387, took: 19.8602s\n  Batch 3099/3907, reward: 5.148, loss: -0.1274, took: 19.7979s\n  Batch 3199/3907, reward: 5.131, loss: -0.1495, took: 19.8352s\n  Batch 3299/3907, reward: 5.137, loss: -0.1279, took: 20.0653s\n  Batch 3399/3907, reward: 5.135, loss: -0.1504, took: 20.2550s\n  Batch 3499/3907, reward: 5.138, loss: -0.1636, took: 20.3127s\n  Batch 3599/3907, reward: 5.133, loss: -0.1261, took: 20.3465s\n  Batch 3699/3907, reward: 5.129, loss: -0.1091, took: 20.3718s\n  Batch 3799/3907, reward: 5.140, loss: -0.1508, took: 20.3295s\n  Batch 3899/3907, reward: 5.138, loss: -0.1381, took: 20.4086s\ntensor([ 9,  5,  6,  3,  1,  4,  0, 10,  8,  7,  0,  2,  0,  0,  0],\n       device='cuda:0')\ntensor([ 2,  4,  6,  3,  0, 10,  1,  8,  5,  9,  0,  7,  0,  0,  0],\n       device='cuda:0')\ntensor([ 7,  6,  9,  2, 10,  0,  8,  4,  3,  0,  1,  5,  0,  0],\n       device='cuda:0')\ntensor([ 2,  9,  7,  1,  5, 10,  0,  4,  8,  6,  0,  3,  0,  0,  0],\n       device='cuda:0')\nMean epoch loss/reward: -0.1472, 5.1385, 5.0757, took: 794.2116s (20.1516s / 100 batches)\n\n  Batch 99/3907, reward: 5.149, loss: -0.1561, took: 20.4324s\n  Batch 199/3907, reward: 5.145, loss: -0.1064, took: 20.3365s\n  Batch 299/3907, reward: 5.138, loss: -0.1684, took: 19.7949s\n  Batch 399/3907, reward: 5.146, loss: -0.1332, took: 19.6772s\n  Batch 499/3907, reward: 5.141, loss: -0.1045, took: 19.7355s\n  Batch 599/3907, reward: 5.137, loss: -0.1496, took: 19.8672s\n  Batch 699/3907, reward: 5.142, loss: -0.1650, took: 20.3326s\n  Batch 799/3907, reward: 5.128, loss: -0.1706, took: 20.4974s\n  Batch 899/3907, reward: 5.125, loss: -0.1419, took: 20.3234s\n  Batch 999/3907, reward: 5.130, loss: -0.1714, took: 20.2821s\n  Batch 1099/3907, reward: 5.133, loss: -0.1615, took: 20.4127s\n  Batch 1199/3907, reward: 5.133, loss: -0.1210, took: 20.4845s\n  Batch 1299/3907, reward: 5.122, loss: -0.1500, took: 20.3602s\n  Batch 1399/3907, reward: 5.131, loss: -0.1288, took: 20.4178s\n  Batch 1499/3907, reward: 5.123, loss: -0.1586, took: 20.2884s\n  Batch 1599/3907, reward: 5.116, loss: -0.1352, took: 20.1442s\n  Batch 1699/3907, reward: 5.120, loss: -0.1689, took: 20.2158s\n  Batch 1799/3907, reward: 5.130, loss: -0.1864, took: 19.9678s\n  Batch 1899/3907, reward: 5.118, loss: -0.1722, took: 19.9686s\n  Batch 1999/3907, reward: 5.131, loss: -0.1511, took: 20.1199s\n  Batch 2099/3907, reward: 5.143, loss: -0.1318, took: 20.2292s\n  Batch 2199/3907, reward: 5.128, loss: -0.1483, took: 19.8353s\n  Batch 2299/3907, reward: 5.117, loss: -0.1365, took: 19.9557s\n  Batch 2399/3907, reward: 5.115, loss: -0.1173, took: 19.9339s\n  Batch 2499/3907, reward: 5.115, loss: -0.1399, took: 19.9510s\n  Batch 2599/3907, reward: 5.115, loss: -0.1331, took: 20.3366s\n  Batch 2699/3907, reward: 5.106, loss: -0.1174, took: 20.0190s\n  Batch 2799/3907, reward: 5.109, loss: -0.1676, took: 20.2223s\n  Batch 2899/3907, reward: 5.112, loss: -0.1382, took: 20.3181s\n  Batch 2999/3907, reward: 5.120, loss: -0.1218, took: 20.2867s\n  Batch 3099/3907, reward: 5.111, loss: -0.1102, took: 20.0125s\n  Batch 3199/3907, reward: 5.105, loss: -0.1733, took: 20.1113s\n  Batch 3299/3907, reward: 5.101, loss: -0.1086, took: 19.7848s\n  Batch 3399/3907, reward: 5.106, loss: -0.1888, took: 19.7512s\n  Batch 3499/3907, reward: 5.108, loss: -0.1588, took: 19.6667s\n  Batch 3599/3907, reward: 5.110, loss: -0.1685, took: 19.8391s\n  Batch 3699/3907, reward: 5.121, loss: -0.1193, took: 19.7320s\n  Batch 3799/3907, reward: 5.105, loss: -0.1357, took: 19.5674s\n  Batch 3899/3907, reward: 5.138, loss: -0.1378, took: 19.8463s\ntensor([ 9,  6,  5,  3,  1,  4,  0,  8, 10,  7,  0,  2,  0,  0],\n       device='cuda:0')\ntensor([ 2,  4,  6,  3,  0,  9, 10,  1,  8,  5,  0,  7,  0,  0,  0],\n       device='cuda:0')\ntensor([ 6,  9,  2,  4,  3,  0,  7, 10,  5,  0,  1,  8,  0,  0,  0],\n       device='cuda:0')\ntensor([ 2,  7,  9,  1,  5, 10,  0,  6,  4,  8,  0,  3,  0,  0,  0],\n       device='cuda:0')\nMean epoch loss/reward: -0.1448, 5.1237, 5.0847, took: 791.2568s (20.0784s / 100 batches)\n\n  Batch 99/3907, reward: 5.155, loss: -0.1565, took: 19.9153s\n  Batch 199/3907, reward: 5.115, loss: -0.1597, took: 19.7072s\n  Batch 299/3907, reward: 5.110, loss: -0.1540, took: 19.8548s\n  Batch 399/3907, reward: 5.117, loss: -0.2101, took: 19.9223s\n  Batch 499/3907, reward: 5.107, loss: -0.1622, took: 19.8929s\n  Batch 599/3907, reward: 5.111, loss: -0.1403, took: 20.0065s\n  Batch 699/3907, reward: 5.117, loss: -0.1422, took: 19.7775s\n  Batch 799/3907, reward: 5.103, loss: -0.1733, took: 19.8549s\n  Batch 899/3907, reward: 5.111, loss: -0.1290, took: 20.5223s\n  Batch 999/3907, reward: 5.122, loss: -0.1116, took: 20.3315s\n  Batch 1099/3907, reward: 5.109, loss: -0.1555, took: 20.3620s\n  Batch 1199/3907, reward: 5.109, loss: -0.1702, took: 20.3895s\n  Batch 1299/3907, reward: 5.112, loss: -0.1031, took: 20.0444s\n  Batch 1399/3907, reward: 5.106, loss: -0.1974, took: 19.7936s\n  Batch 1499/3907, reward: 5.116, loss: -0.0770, took: 19.5931s\n  Batch 1599/3907, reward: 5.099, loss: -0.1220, took: 19.5775s\n  Batch 1699/3907, reward: 5.103, loss: -0.1110, took: 19.6071s\n  Batch 1799/3907, reward: 5.111, loss: -0.1263, took: 19.7162s\n  Batch 1899/3907, reward: 5.097, loss: -0.1582, took: 19.6888s\n  Batch 1999/3907, reward: 5.102, loss: -0.1422, took: 19.7249s\n  Batch 2099/3907, reward: 5.106, loss: -0.1644, took: 19.6917s\n  Batch 2199/3907, reward: 5.108, loss: -0.1317, took: 20.4067s\n  Batch 2299/3907, reward: 5.106, loss: -0.1171, took: 19.8084s\n  Batch 2399/3907, reward: 5.107, loss: -0.1489, took: 19.8648s\n  Batch 2499/3907, reward: 5.099, loss: -0.1434, took: 19.8834s\n  Batch 2599/3907, reward: 5.090, loss: -0.1516, took: 20.0322s\n  Batch 2699/3907, reward: 5.095, loss: -0.1502, took: 19.9008s\n  Batch 2799/3907, reward: 5.103, loss: -0.1548, took: 19.6904s\n  Batch 2899/3907, reward: 5.099, loss: -0.1656, took: 19.8549s\n  Batch 2999/3907, reward: 5.090, loss: -0.1874, took: 19.7640s\n  Batch 3099/3907, reward: 5.090, loss: -0.1631, took: 20.1597s\n  Batch 3199/3907, reward: 5.087, loss: -0.1410, took: 19.6818s\n  Batch 3299/3907, reward: 5.078, loss: -0.1656, took: 19.7653s\n  Batch 3399/3907, reward: 5.092, loss: -0.1340, took: 19.6313s\n  Batch 3499/3907, reward: 5.097, loss: -0.1258, took: 19.6411s\n  Batch 3599/3907, reward: 5.095, loss: -0.1642, took: 19.6928s\n  Batch 3699/3907, reward: 5.105, loss: -0.1485, took: 19.7667s\n  Batch 3799/3907, reward: 5.118, loss: -0.1664, took: 19.7396s\n  Batch 3899/3907, reward: 5.136, loss: -0.1626, took: 20.2079s\ntensor([ 9,  6,  5,  3,  1,  4,  0, 10,  8,  7,  0,  2,  0,  0,  0],\n       device='cuda:0')\ntensor([ 2,  4,  6,  3,  0, 10,  1,  8,  5,  9,  0,  7,  0,  0,  0],\n       device='cuda:0')\ntensor([ 7,  6,  9,  2, 10,  0,  3,  4,  5,  0,  8,  1,  0,  0,  0],\n       device='cuda:0')\ntensor([ 2,  7,  9,  5,  4, 10,  0,  1,  6,  8,  0,  3,  0,  0,  0],\n       device='cuda:0')\nMean epoch loss/reward: -0.1482, 5.1060, 5.0730, took: 784.1404s (19.8837s / 100 batches)\n\ntensor([ 5,  7,  4,  6,  2,  1,  0,  9,  3,  8,  0, 10,  0,  0,  0],\n       device='cuda:0')\ntensor([ 8,  7,  5, 10,  6,  1,  2,  0,  3,  9,  4,  0,  0,  0,  0],\n       device='cuda:0')\ntensor([ 5, 10,  9,  8,  0,  2,  3,  4,  7,  0,  6,  1,  0,  0,  0],\n       device='cuda:0')\ntensor([ 2,  4,  7,  5,  1,  3,  9,  0,  8, 10,  6,  0,  0,  0],\n       device='cuda:0')\nAverage tour length:  5.091183304786682\nvrp/10/20_38_59.265263/actor.pt\nStarting VRP testing\ntensor([[[0.5508],\n         [0.3597]]], device='cuda:0')\ntensor([[[0.5508, 0.4419, 0.3837, 0.4841, 0.5721, 0.9528, 0.0000, 1.0000,\n          0.6946, 0.0183, 0.3741],\n         [0.3597, 0.8190, 1.0000, 0.0245, 0.3496, 0.3837, 0.2337, 0.2345,\n          0.0000, 0.4989, 0.3444]]], device='cuda:0')\ntensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n          1.0000, 1.0000, 1.0000],\n         [0.0000, 0.2160, 0.3560, 0.3600, 0.4240, 0.2960, 0.2520, 0.2920,\n          0.0440, 0.3600, 0.2440]]], device='cuda:0')\n/kaggle/input/vrp-rl5/project2_2/Tasks/vrp.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return torch.tensor(tensor.data, device=dynamic.device)\nComputed tour indices: [[6, 9, 1, 8, 0, 10, 2, 5, 0, 3, 7, 0, 4, 0]]\ntensor([[ 6,  9,  1,  8,  0, 10,  2,  5,  0,  3,  7,  0,  4,  0]],\n       device='cuda:0')\nTotal distance traveled: 6.093073820515615\nStarting VRP testing with random data\ntensor([[[0.8129, 0.3625, 0.1440, 0.0093, 0.7604, 0.0438, 0.9267, 0.1096,\n          0.6766, 0.9658, 0.5361],\n         [0.7143, 0.4080, 0.1867, 0.1991, 0.2802, 0.8797, 0.4233, 0.5033,\n          0.1648, 0.5712, 0.6100]]], device='cuda:0')\ntensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n          1.0000, 1.0000, 1.0000],\n         [0.0000, 0.1840, 0.2200, 0.3960, 0.2000, 0.0880, 0.1720, 0.1200,\n          0.3440, 0.1720, 0.1680]]], device='cuda:0')\ntensor([[[0.8129],\n         [0.7143]]], device='cuda:0')\nComputed tour indices: [[1, 2, 3, 7, 0, 5, 10, 8, 4, 6, 0, 9, 0]]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Detected device {}'.format(device))","metadata":{"execution":{"iopub.status.busy":"2024-07-17T22:53:51.482481Z","iopub.execute_input":"2024-07-17T22:53:51.483488Z","iopub.status.idle":"2024-07-17T22:53:53.246312Z","shell.execute_reply.started":"2024-07-17T22:53:51.483445Z","shell.execute_reply":"2024-07-17T22:53:53.245351Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Detected device cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/input/vrp-rl5/project2_2')\nfrom main import test_vrp\nfrom Models.actor import DRL4VRP\nfrom Tasks.vrp import VehicleRoutingDataset\n\nactor_checkpoint = '/kaggle/working/vrp/10/20_38_59.265263/actor.pt'\n\ndataset = VehicleRoutingDataset(1000000, 10)\n\n#Establish some features of dataset \ndataset.static_size = 2\ndataset.dynamic_size = 2\n\n# Define and load model actor\nactor = DRL4VRP(dataset.static_size, dataset.dynamic_size, 128, \n                    dataset.update_dynamic, dataset.update_mask, num_layers=1, dropout=0.1).to(device)\nactor.load_state_dict(torch.load(actor_checkpoint, map_location=device))\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-17T22:54:51.743961Z","iopub.execute_input":"2024-07-17T22:54:51.744309Z","iopub.status.idle":"2024-07-17T22:54:52.884471Z","shell.execute_reply.started":"2024-07-17T22:54:51.744283Z","shell.execute_reply":"2024-07-17T22:54:52.883596Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Detected device cuda\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"test_vrp('/kaggle/input/vrp-rl5/project2_2/input.txt',actor)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T23:11:30.542986Z","iopub.execute_input":"2024-07-17T23:11:30.543821Z","iopub.status.idle":"2024-07-17T23:11:30.772690Z","shell.execute_reply.started":"2024-07-17T23:11:30.543782Z","shell.execute_reply":"2024-07-17T23:11:30.771726Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Starting VRP testing\ntensor([[[0.5508],\n         [0.3597]]], device='cuda:0')\ntensor([[[0.5508, 0.4419, 0.3837, 0.4841, 0.5721, 0.9528, 0.0000, 1.0000,\n          0.6946, 0.0183, 0.3741],\n         [0.3597, 0.8190, 1.0000, 0.0245, 0.3496, 0.3837, 0.2337, 0.2345,\n          0.0000, 0.4989, 0.3444]]], device='cuda:0')\ntensor([[[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n          1.0000, 1.0000, 1.0000],\n         [0.0000, 0.2160, 0.3560, 0.3600, 0.4240, 0.2960, 0.2520, 0.2920,\n          0.0440, 0.3600, 0.2440]]], device='cuda:0')\nComputed tour indices: [[6, 9, 1, 8, 0, 10, 2, 5, 0, 3, 7, 0, 4, 0]]\ntensor([[ 6,  9,  1,  8,  0, 10,  2,  5,  0,  3,  7,  0,  4,  0]],\n       device='cuda:0')\n","output_type":"stream"},{"name":"stderr","text":"/kaggle/input/vrp-rl5/project2_2/Tasks/vrp.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return torch.tensor(tensor.data, device=dynamic.device)\n","output_type":"stream"},{"name":"stdout","text":"Total distance traveled: 6.093073820515615\n","output_type":"stream"}]}]}